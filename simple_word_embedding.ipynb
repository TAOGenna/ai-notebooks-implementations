{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple word embedding experiment\n",
    "\n",
    "See:\n",
    "- Pag 386 of Data Science from Scratch\n",
    "\n",
    "## Plan\n",
    "- data\n",
    "    - hand-made sentences: what is the basic structure of a sentence? '\"the\" + color + noun + verb + adverb + adjective.'\n",
    "    - what is the thing we predict and what is our target? predict: an embedding which is just a collection of numbers that it tries to adjust to the target | it serves as a proxy to improve the parameters of our embedding matrix, target: an embedding belonging to a specific word in our vocabulary.\n",
    "- architecture\n",
    "    - embedding layer + linear layer + softmax cross entropy\n",
    "    - i imagine pytorch is going to take care of the gradients \n",
    "## Data\n",
    "Our data is going to be handmade. We will feed our network in training pairs `(word,nearby_word)` and try to minimize the `SoftmaxCrossEntropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the red car was quite fast .',\n",
       " 'the yellow dog looks quite slow .',\n",
       " 'the yellow boat seems extremely slow .',\n",
       " 'the blue car was extremely slow .',\n",
       " 'the blue car is quite fast .',\n",
       " 'the yellow boat seems quite fast .',\n",
       " 'the white house seems absurdly small .',\n",
       " 'the red dog is extremely slow .',\n",
       " 'the white boat looks quite big .',\n",
       " 'the white house seems quite big .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# let's start with the data\n",
    "# we will create some random sentences with:\n",
    "color = ['blue','red','green','yellow','white']\n",
    "noun  = ['cat','dog','car','boat','house']\n",
    "verb  = ['is','was','seems','looks']\n",
    "adverb = ['quite','absurdly','extremely']\n",
    "adjective = ['slow','fast','big','small']\n",
    "\n",
    "# joining all these words following \"the\" + color + noun + verb + adverb + adjective.\n",
    "num_sentences = 100\n",
    "NUM_WORDS = len(color)+len(noun)+len(verb)+len(adverb)+len(adjective)\n",
    "\n",
    "sentences = [\" \".join(['the',random.choice(color),random.choice(noun),random.choice(verb),random.choice(adverb),random.choice(adjective),'.']) for _ in range(num_sentences)]\n",
    "random.sample(sentences,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# now we should assign indexes to words and viceversa to have some sort of mapping \n",
    "idx_to_word = {}\n",
    "word_to_idx = {}\n",
    "\n",
    "def get_one_hot(word):\n",
    "    return [1.0 if i==word_to_idx[word] else 0.0 for i in range(NUM_WORDS)]\n",
    "\n",
    "joined_words = color + noun + verb + adverb + adjective\n",
    "for idx, word in enumerate(joined_words):\n",
    "    idx_to_word[idx] = word\n",
    "    word_to_idx[word] = idx\n",
    "\n",
    "# we use redex to tokenize the senteces and obtain the training pairs\n",
    "training_pairs = []\n",
    "for sentence in sentences:\n",
    "    tokenized_sentece = re.findall(r'\\b\\w+\\b',sentence) # the `r` in r'\\b\\w+\\b' indicates that the string should we treated as a raw string (because of the \\)\n",
    "    for i in range(len(tokenized_sentece)-1):\n",
    "            training_pairs.append((tokenized_sentece[i],get_one_hot(tokenized_sentece[i+1])))\n",
    "\n",
    "# sanity check\n",
    "assert get_one_hot(training_pairs[1][0]) == [1.0 if i==word_to_idx[training_pairs[1][0]] else 0.0 for i in range(NUM_WORDS)]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/word_embedding_architecture.jpg\" alt=\"image.png\" style=\"width: 50%;\"/>\n",
    "    <figcaption>simple word embedding architecture.</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    # if all goes well the autograd is going to do what I wish it would do \n",
    "    def __init__(self,word_elements,embed_dimension):\n",
    "        super().__init__()\n",
    "        # initialize tensor with normal distribution of shape emb = (num_words,embed_dimension)\n",
    "        self.embedding = nn.Parameter(torch.randn((word_elements, embed_dimension), requires_grad=True))\n",
    "    \n",
    "    def forward(self,word):\n",
    "        \"\"\"\n",
    "        word: word string\n",
    "        we will get the string id and return its embedding\n",
    "        \"\"\"\n",
    "        return self.embedding[word_to_idx[word]]\n",
    "\n",
    "# include the linear layer in our model | careful with the dimensions\n",
    "EMBEDDING_SIZE = 10  \n",
    "model = nn.Sequential(\n",
    "    Embedding(NUM_WORDS, EMBEDDING_SIZE),\n",
    "    nn.Linear(EMBEDDING_SIZE, NUM_WORDS) \n",
    ")\n",
    "\n",
    "# Training loop\n",
    "epochs = len(training_pairs)\n",
    "optim = nn.optim.Adam(model.parameters, lr=0.001, weight_decay=1e-2)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# keep track of loss function\n",
    "losses = [] \n",
    "accuracies = []\n",
    "\n",
    "for i in (t := trange(epochs)):\n",
    "    X,target = training_pairs\n",
    "    optim.zero_grad()\n",
    "    prediction = model(X)\n",
    "    loss = loss_function(prediction,target)# order matters for the Cross Entropy Loss\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    loss, accuracy = loss.item(), accuracy.item()\n",
    "    losses.append(loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    t.set_description(f\"Loss: {loss:.2f}, accuracy: {accuracy:.2f}\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.plot(accuracies, alpha = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
