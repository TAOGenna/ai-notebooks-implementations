{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple word embedding experiment\n",
    "\n",
    "See:\n",
    "- Pag 386 of Data Science from Scratch\n",
    "\n",
    "## Plan\n",
    "- data\n",
    "    - hand-made sentences: what is the basic structure of a sentence? '\"the\" + color + noun + verb + adverb + adjective.'\n",
    "    - what is the thing we predict and what is our target? predict: an embedding which is just a collection of numbers that it tries to adjust to the target | it serves as a proxy to improve the parameters of our embedding matrix, target: an embedding belonging to a specific word in our vocabulary.\n",
    "- architecture\n",
    "    - embedding layer + linear layer + softmax cross entropy\n",
    "    - i imagine pytorch is going to take care of the gradients \n",
    "## Data\n",
    "Our data is going to be handmade. We will feed our network in training pairs `(word,nearby_word)` and try to minimize the `SoftmaxCrossEntropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the white cat is absurdly fast .',\n",
       " 'the red house seems quite small .',\n",
       " 'the red dog looks extremely slow .',\n",
       " 'the red boat seems absurdly slow .',\n",
       " 'the red dog seems quite small .',\n",
       " 'the red cat seems extremely fast .',\n",
       " 'the green house seems extremely small .',\n",
       " 'the red cat looks extremely fast .',\n",
       " 'the blue car seems absurdly small .',\n",
       " 'the green dog looks extremely big .']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "# let's start with the data\n",
    "# we will create some random sentences with:\n",
    "color = ['blue','red','green','yellow','white']\n",
    "noun  = ['cat','dog','car','boat','house']\n",
    "verb  = ['is','was','seems','looks']\n",
    "adverb = ['quite','absurdly','extremely']\n",
    "adjective = ['slow','fast','big','small']\n",
    "\n",
    "# joining all these words following \"the\" + color + noun + verb + adverb + adjective.\n",
    "num_sentences = 100\n",
    "\n",
    "sentences = [\" \".join(['the',random.choice(color),random.choice(noun),random.choice(verb),random.choice(adverb),random.choice(adjective),'.']) for _ in range(num_sentences)]\n",
    "random.sample(sentences,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/word_embedding_architecture.jpg\" alt=\"image.png\" style=\"width: 50%;\"/>\n",
    "    <figcaption>simple word embedding architecture.</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'green'), ('green', 'cat'), ('cat', 'looks'), ('looks', 'absurdly'), ('absurdly', 'big')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# now we should assign indexes to words and viceversa to have some sort of mapping \n",
    "idx_to_word = {}\n",
    "word_to_idx = {}\n",
    "\n",
    "joined_words = color + noun + verb + adverb + adjective\n",
    "for idx, word in enumerate(joined_words):\n",
    "    idx_to_word[idx] = word\n",
    "    word_to_idx[word] = idx\n",
    "\n",
    "training_pairs = []\n",
    "# we use redex to tokenize the senteces and obtain the training pairs\n",
    "for sentence in sentences:\n",
    "    tokens = re.findall(r'\\b\\w+\\b',sentence) # the `r` in r'\\b\\w+\\b' indicates that the string should we treated as a raw string (because of the \\)\n",
    "    for i in range(len(tokens)-1):\n",
    "            training_pairs.append((tokens[i],tokens[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,num_words,embed_dimension):\n",
    "        # Where do I specify that this thing needs grad? \n",
    "        # initialize tensor with normal distribution of shape emb = (num_words,embed_dimension)\n",
    "        pass\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # we should refer to the embedding belonging to the word x, we should map words to indexes\n",
    "        # return emb[id[x]]\n",
    "        pass\n",
    "\n",
    "# include the linear layer in our model \n",
    "model = nn.Sequential(\n",
    "    Embedding(),\n",
    "    nn.Linear(embed_dimension,embed_dimension)\n",
    ")\n",
    "\n",
    "epochs = num_words\n",
    "optim = nn.optim.Adam([TODO]) # TODO: put parameteres of the model: embedding and linear\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "for _ in range(epochs): # TODO: use tqdm \n",
    "    # set optimizer grad to zero\n",
    "    out = #TODO:  calculate output of the model\n",
    "    loss_calc = loss(out,...) # TODO: add target ...\n",
    "    optim.step()\n",
    "\n",
    "    #TODO: add stuff to visualize progress bar\n",
    "\n",
    "# TODO: one it is trained, plot everything in 2D to see if it actually learned something about the semantics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
