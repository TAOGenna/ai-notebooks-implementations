{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: reproduce nanoGPT tuto from Karpathy\n",
    "**plan:**\n",
    "- first implement a word embedder\n",
    "    - [x] I already got the data(Shakespeare books)\n",
    "    - [x] read that data\n",
    "    - [ ] tokenize the data\n",
    "    - [ ] create an embedder layer \n",
    "    - [ ] generator: generate new text: how do I recover a letter based on the output of my network? \n",
    "\n",
    "- modify the code to support an attention mechanism\n",
    "\n",
    "---\n",
    "\n",
    "as i am coding, I sometimes cheat and see karpathy's code. I tried just to cheat little things like some pythonic way to write something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# printing first 1000 characters of input.txt\n",
    "with open(\"input.txt\", 'r') as file:\n",
    "    text = file.read()\n",
    "    print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# since the idea is to create a character prediction program I guess we should tokenize the characters, not words \n",
    "\n",
    "# grab all the different characters in our text\n",
    "# from karpathy: - sorted - print by joining - vocab_size (should be useful later) \n",
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "print(''.join(characters))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "[46, 43, 50, 50, 3, 3, 53, 1, 39, 57, 42, 1, 57]\n",
      "hell$$o asd s\n"
     ]
    }
   ],
   "source": [
    "# I guess this map can account for an embedder \n",
    "# from karpathy: -encode -decode, I was doing it for a single character or integer, it should be for a list of such. plus, for the decoder we should output a string, not a list of characters\n",
    "stoi = {elem:i for i,elem in enumerate(characters)} # stoi: string to index \n",
    "itos = {i:elem for i,elem in enumerate(characters)} # \n",
    "encode = lambda element: [stoi[elem] for elem in element]\n",
    "decode = lambda element: ''.join([itos[elem] for elem in element])\n",
    "print(stoi)\n",
    "print(encode('hell$$o asd s'))\n",
    "print(decode(encode('hell$$o asd s')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Andrej goes on to say that this is heavily based on Bigram.\n",
    "What is Bigram? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58]\n",
      "for the context[18] be have the target 47\n",
      "for the context[18, 47] be have the target 56\n",
      "for the context[18, 47, 56] be have the target 57\n",
      "for the context[18, 47, 56, 57] be have the target 58\n",
      "for the context[18, 47, 56, 57, 58] be have the target 1\n",
      "for the context[18, 47, 56, 57, 58, 1] be have the target 15\n",
      "for the context[18, 47, 56, 57, 58, 1, 15] be have the target 47\n",
      "for the context[18, 47, 56, 57, 58, 1, 15, 47] be have the target 58\n"
     ]
    }
   ],
   "source": [
    "# - work with batches\n",
    "# - work with blocks \n",
    "\n",
    "block_size = 8\n",
    "\n",
    "block = encode(text[:block_size+1])\n",
    "print(block)\n",
    "for i in range(block_size):\n",
    "    foo = block[:i+1]\n",
    "    target = block[i+1]\n",
    "    print(f'for the context{foo} be have the target {target}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to partition our data\n",
    "# and work in batches\n",
    "# meaning we will have a collection of `batch_size` random numbers (i.e positions in the text array) and for each of these positions we will\n",
    "# get a `block`\n",
    "import torch\n",
    "\n",
    "train_size = len(text)*0.9\n",
    "train_data = text[:train_size]\n",
    "dev_data = text[train_size:]\n",
    "def generate_data(stage):\n",
    "    data = train_data if stage=='train' else dev_data \n",
    "    positions = torch.randint()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
